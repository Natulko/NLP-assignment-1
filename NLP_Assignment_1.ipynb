{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2601675e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "# Install NLTK\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f1b4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/natulko/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the Brown Corpus\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "\n",
    "# Imports\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d016672",
   "metadata": {},
   "source": [
    "# Natural Language Processing Assignment 1: The Notebook\n",
    "\n",
    "This is the notebook for the first hand-in assignment for Natural Language Processing. The notebook counts for 50% of the total assignment, which counts towards 15% of the final grade. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dd44fb",
   "metadata": {},
   "source": [
    "## Assignment 5 (20 points)\n",
    "\n",
    "In the exercises you had the opportunity to practice with fitting $n$-gram language models on the Brown corpus. In this assignment you will fit an interpolated model and learn how to avoid zero counts by exploiting a heldout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5fa788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports + code from exercises\n",
    "from nltk.corpus import brown\n",
    "import random\n",
    "from nltk.lm import MLE, Laplace\n",
    "from nltk.lm.api import LanguageModel\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "def split_data(data, train_ratio: float, heldout_ratio: float):\n",
    "    random.Random(42).shuffle(data)\n",
    "    train_cutoff = int(train_ratio*len(data))\n",
    "    heldout_cutoff = int((train_ratio+heldout_ratio)*len(data))\n",
    "    return {'train': data[:train_cutoff], 'heldout': data[train_cutoff:heldout_cutoff], 'test': data[heldout_cutoff:]}\n",
    "\n",
    "# data: a list of (unpadded) sentences that are split into words\n",
    "# n: the maximum order of n-grams to include\n",
    "# model_type: parameter to support different types of language models\n",
    "def fit_language_model(data: List[List[str]], n: int, model_type=MLE):\n",
    "    train, vocab = padded_everygram_pipeline(n, data) \n",
    "    model = model_type(n)\n",
    "    model.fit(train, vocab)\n",
    "    return model\n",
    "\n",
    "def evaluate_language_model(model: LanguageModel, test_data: List[str], n: int):\n",
    "    # Note that `padded_everygram_pipeline' adds n-1 SOS and EOS symbols\n",
    "    # which trickles through in calculating perplexity. \n",
    "    # For example, padding for n=3 will add the bigram for (<s>, <s>) which is NOT present in the model for n=2.\n",
    "    gram_data, _ = padded_everygram_pipeline(n, test_data)\n",
    "    test_grams = [x for t in gram_data for x in model.vocab.lookup(t) if len(x)==n]\n",
    "    return model.perplexity(test_grams)\n",
    "\n",
    "# Prepare data\n",
    "non_fiction = brown.sents(categories=['editorial', 'government', 'learned', 'news', 'reviews'])\n",
    "non_fiction = [s for s in non_fiction if len(s)>=3]\n",
    "\n",
    "fiction = brown.sents(categories=['adventure', 'fiction', 'lore', 'mystery', 'romance', 'science_fiction'])\n",
    "fiction = [s for s in fiction if len(s)>=3]\n",
    "\n",
    "fiction_data = split_data(fiction, 0.9, 0.05)\n",
    "non_fiction_data = split_data(non_fiction, 0.9, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110ce005",
   "metadata": {},
   "source": [
    "### Part 5.1: training a language model (10 points)\n",
    "\n",
    "You may recall from the exercises that a simple smoothed language model doesn't perform so well on a test set, and generates strange examples. In this assignment you will investigate the use of an interpolated language model, combining it with parameters extracted from a heldout set.\n",
    "\n",
    "Given are the following data: like in the exercises, we work with the Brown corpus and specifically the fiction and non-fiction texts of the corpus. Moreover, we will fix all data to contain all $n$-grams up to $n=3$.\n",
    "\n",
    "As a first step, make sure you have fiction and non-fiction models available with Laplace smoothing (you may have fit these in the exercises already). Then, fit similar models but now use the `AbsoluteDiscountingInterpolated` model type, that implements absolute discounting. Calculate the perplexity to realize that once again there will be zero counts because the model is unsmoothed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a475a4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiction model perplexity: 2808.092983899774\n",
      "Non-fiction model perplexity: 4488.435817363989\n",
      "Fiction model (ADI) perplexity: inf\n",
      "Non-fiction model (ADI) perplexity: inf\n"
     ]
    }
   ],
   "source": [
    "# Part 5.1 SOLUTION:\n",
    "from nltk.lm import AbsoluteDiscountingInterpolated\n",
    "\n",
    "# make sure you have fiction and non-fiction models available with Laplace smoothing\n",
    "fiction_model = fit_language_model(fiction_data['train'], 2, Laplace)\n",
    "non_fiction_model = fit_language_model(non_fiction_data['train'], 2, Laplace)\n",
    "\n",
    "# fit similar models with AbsoluteDiscountingInterpolated\n",
    "fiction_model_adi = fit_language_model(fiction_data['train'], 2, AbsoluteDiscountingInterpolated)\n",
    "non_fiction_model_adi = fit_language_model(non_fiction_data['train'], 2, AbsoluteDiscountingInterpolated)\n",
    "\n",
    "# calculate perplexity\n",
    "fiction_perplexity = evaluate_language_model(fiction_model, fiction_data['test'], 2)\n",
    "non_fiction_perplexity = evaluate_language_model(non_fiction_model, non_fiction_data['test'], 2)\n",
    "\n",
    "fiction_perplexity_adi = evaluate_language_model(fiction_model_adi, fiction_data['test'], 2)\n",
    "non_fiction_perplexity_adi = evaluate_language_model(non_fiction_model_adi, non_fiction_data['test'], 2)\n",
    "\n",
    "print(f'Fiction model perplexity: {fiction_perplexity}')\n",
    "print(f'Non-fiction model perplexity: {non_fiction_perplexity}')\n",
    "print(f'Fiction model (ADI) perplexity: {fiction_perplexity_adi}')\n",
    "print(f'Non-fiction model (ADI) perplexity: {non_fiction_perplexity_adi}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1689bd9",
   "metadata": {},
   "source": [
    "As we can see, indeed without Laplace's smoothing we get infinite perplexity, as ... # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41afef4f",
   "metadata": {},
   "source": [
    "### Part 5.2: avoiding zero counts (10 points)\n",
    "\n",
    "One way of avoiding zero counts is to make sure that unseen words are mapped to a special token `<UNK>` (unknown). But how do we quantify the $n$-gram probability of unknown words? Well, we can use the held out set to estimate these counts. The implementation of language models in NLTK then allows us to simply *update* a language model with more data.\n",
    "\n",
    "Hence, your assignment here is to implement the method `update_model` below. It takes three inputs: a language model, the heldout data, and the parameter `n` for the maximum $n$-grams to include. What the method should do:\n",
    "\n",
    " 1. Use the language model's vocabulary to map unseen words of the heldout data to the `<UNK>` token,\n",
    " 2. Generate the $n$-grams for the (tokenized) heldout data,\n",
    " 3. Update the language model with these counts\n",
    "\n",
    "After successful implementation of the method, use it to update the interpolated model and calculate its perplexity. How does it compare to the perplexity of the Laplace smoothed model?\n",
    "\n",
    "*Hint: you may take inspiration from the method `evaluate_language_model` above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ac22c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiction model (ADI) perplexity after update: 237.51032877960017\n",
      "Non-fiction model (ADI) perplexity after update: 311.7907156649726\n"
     ]
    }
   ],
   "source": [
    "# Part 5.2 SOLUTION:\n",
    "\n",
    "def update_model(model: LanguageModel, heldout_data: List[List[str]], n: int):\n",
    "    heldout_data = [[word if word in model.vocab else '<UNK>' for word in sent] for sent in heldout_data]\n",
    "    heldout_grams, _ = padded_everygram_pipeline(n, heldout_data)\n",
    "    model.counts.update(heldout_grams)\n",
    "    return model\n",
    "\n",
    "# update interpolated model\n",
    "fiction_model_adi = update_model(fiction_model_adi, fiction_data['heldout'], 2)\n",
    "non_fiction_model_adi = update_model(non_fiction_model_adi, non_fiction_data['heldout'], 2)\n",
    "\n",
    "# calculate perplexity\n",
    "fiction_perplexity_adi = evaluate_language_model(fiction_model_adi, fiction_data['test'], 2)\n",
    "non_fiction_perplexity_adi = evaluate_language_model(non_fiction_model_adi, non_fiction_data['test'], 2)\n",
    "\n",
    "print(f'Fiction model (ADI) perplexity after update: {fiction_perplexity_adi}')\n",
    "print(f'Non-fiction model (ADI) perplexity after update: {non_fiction_perplexity_adi}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9b8b1",
   "metadata": {},
   "source": [
    "## Assignment 6: part-of-speech tagging (20 points)\n",
    "\n",
    "This assignment consists of two parts in which you will consider parts-of-speech (POS) and POS tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682096b",
   "metadata": {},
   "source": [
    "### Part 6.1: Ambiguity in open and closed class words (10 points)\n",
    "\n",
    "In the lecture and in the exercises you saw that a small portion of words is ambiguous, but they nevertheless cover a large portion of text corpora. You also learnt about the distinction between open (i.e. dynamic words that may appear and disappear over time) and closed class words (i.e. pretty much fixed sets of words).\n",
    "\n",
    "Below are two lists: one corresponds to closed class words, and another corresponds to open class words. We wish to inspect the ambiguity of words that are open class and words that are open class, and figure out whether words can belong to both!\n",
    "\n",
    "Your task is as follows:\n",
    "1. Determine the percentage of *unique* words that occur both as open class and as closed class words. I.e. out of all *unique* words, how many of them have been tagged at least once as open and and at least once as closed class?\n",
    "2. For the *unique* words that only occur as either open class words or closed class words, calculate the percentage of *unique* words in each class that is may receive multiple POS tags (in their respective class). Do you find that open class words are more ambiguous, or less ambiguous then closed class words?\n",
    "\n",
    "*Hint: If you run into any performance issues because of iterating over large lists, remember that you can convert a list into a set first and then membership lookup in Python becomes linear.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6fc9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "all_word_tags = [word_tag for tagged_sentence in treebank.tagged_sents() for word_tag in tagged_sentence]\n",
    "closed_class_tags = [',', '$', '.', 'CD', 'CC', 'RP', \"''\", '``', 'IN', 'TO',':', 'DT', 'WP', 'PRP',\n",
    "                     'SYM', 'EX', 'PRP$', 'POS', 'MD']\n",
    "open_class_tags = ['JJ', 'JJR', 'JJS', 'VBG', 'VBN', 'VB', 'NNP', 'VBD', 'NN', 'NNS',  'VBZ', 'VBP', 'NNPS']\n",
    "other_tags = ['RBR', '-LRB-', 'PDT', 'WRB', 'WDT', 'RB', 'LS', 'WP$',  'UH', \n",
    "              'FW', '-RRB-',  'RBS', '#', '-NONE-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb5133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 SOLUTION:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74abe745",
   "metadata": {},
   "source": [
    "### Part 6.2: Smoothing a tagger (10 points)\n",
    "\n",
    "In the exercises you trained an HMM tagger that used `LidstoneProbDist` as an estimator for its probability distributions. This estimator generally applies some form of smoothing for its probability calculation. In general this would mean that the probability of a transition from one tag to the other is defined as:\n",
    "\n",
    "$$ P_{\\beta}(t_i | t_{i-1}) = \\frac{C(t_{i-1} t_i) + \\beta}{C(t_{i-1}) + \\beta \\cdot V} $$\n",
    "        \n",
    "where $V$ is the number of possible tags. A similar smoothing applies to the calculation of emission probabilities $P(w_i | t_i)$.\n",
    "\n",
    "In the exercises the smoothing parameter was set as $\\beta=0.1$.\n",
    "\n",
    "In this assignment, you will try to find a better parameter on a given dataset. The code below loads in a portion of the Penn Treebank and has code for training and testing a Hidden Markov Model tagger.\n",
    "\n",
    "Your task is the following:\n",
    "\n",
    "1. Train a tagger with smoothing parameter $0.1$ and verify that its accuracy is $0.917$,\n",
    "2. Train an unsmoothed tagger, i.e. the smoothing parameter should be set to $0$, and report its accuracy,\n",
    "3. Now find a smoothing value for which the corresponding model reaches an accuracy greater than $0.917$. Is it higher or lower than $0.1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e47e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import LidstoneProbDist\n",
    "from nltk.tag.hmm import load_pos, HiddenMarkovModelTrainer, HiddenMarkovModelTagger\n",
    "from nltk.lm import MLE, Laplace\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_hmm_tagger(train_data: List[List[Tuple[str, str]]],\n",
    "                     estimator=lambda fd, bins: LidstoneProbDist(fd, 0.1, bins)) -> HiddenMarkovModelTagger:\n",
    "    tags = list(set([t for s in train_data for w,t in s]))\n",
    "    words = list(set([w for s in train_data for w,t in s]))\n",
    "    trainer = HiddenMarkovModelTrainer(tags, words)\n",
    "    hmm = trainer.train_supervised(train_data, estimator=estimator)\n",
    "    return hmm\n",
    "\n",
    "def test_hmm_tagger(tagger: HiddenMarkovModelTagger, test_data: List[List[Tuple[str, str]]]) -> List[List[Tuple[str, str]]]:\n",
    "    return [tagger.tag([w for w,t in s]) for s in tqdm(test, position=0)]\n",
    "\n",
    "def accuracy(trues: List[str], predictions: List[str]):\n",
    "    return sum([t == p for t, p in zip(trues, predictions)])/len(trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da75bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "import random\n",
    "\n",
    "def split_data(data, cutoff=0.98):\n",
    "    random.Random(42).shuffle(data)\n",
    "    return data[:int(cutoff*len(data))], data[int(cutoff*len(data)):]\n",
    "\n",
    "train, test = split_data(list(treebank.tagged_sents()), cutoff=0.9)\n",
    "display(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d8d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 SOLUTION:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1275c398",
   "metadata": {},
   "source": [
    "## Assignment 7: parsing (20 points)\n",
    "\n",
    "In this assignment, you will perform evaluation over a constituency parser. The first bit of code is taken from the exercises and sets up a probabilistic context-free grammar and a Viterbi parser for it, over a sample from the Penn Treebank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafb64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree.tree import Tree\n",
    "from nltk.corpus import treebank\n",
    "from nltk.grammar import Nonterminal, induce_pcfg\n",
    "from nltk.parse.viterbi import ViterbiParser\n",
    "\n",
    "parsed_sents = treebank.parsed_sents()\n",
    "grammar = induce_pcfg(Nonterminal(\"S\"), [p for s in parsed_sents for p in s.productions()])\n",
    "viterbi_parser = ViterbiParser(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c3938c",
   "metadata": {},
   "source": [
    "The code below loads and displays two example test trees (for the sentences `They will join the board .` and `The student will do the work on Monday .`) containing a few parsing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea9bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example_trees.txt', 'r') as in_file:\n",
    "    example_trees = [Tree.fromstring(s.strip()) for s in in_file.readlines()]\n",
    "\n",
    "display(example_trees[0])\n",
    "display(example_trees[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01058f7f",
   "metadata": {},
   "source": [
    "The code below will compute the span representation for a given tree, as a first step in calculating parser evaluation metrics (precision, recall, F1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e6a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "def get_leaf_position_for_idx(tree, idx):\n",
    "    return [i for i in range(len(tree.leaves())) if tree.leaf_treeposition(i) == idx][0]\n",
    "\n",
    "def get_span_for_index(tree: Tree, idx: Tuple[int,...]):\n",
    "    return [get_leaf_position_for_idx(tree, p) for p in tree.treepositions() \n",
    "            if p[:len(idx)] == idx and isinstance(tree[p], str)]\n",
    "\n",
    "def get_internal_node_idxs(tree: Tree) -> List[Tuple[int,...]]:\n",
    "    return [i for i in tree.treepositions() if isinstance(tree[i], Tree) and tree[i].height() > 1]\n",
    "    \n",
    "def compute_labeled_span_for_node(tree: Tree, idx: Tuple[int,...]) -> Tuple[str, int, int]:\n",
    "    label = tree[idx].label()\n",
    "    span = get_span_for_index(tree, idx)\n",
    "    return label, span[0], span[-1]\n",
    "\n",
    "def string_for_lspan(label: str, left: int, right: int):\n",
    "    return f\"{label}({left},{right})\"\n",
    "\n",
    "def get_labelled_spans_for_tree(tree: Tree) -> List[Tuple[str, int, int]]:\n",
    "    # First grab the tree indices for internal nodes     \n",
    "    internal_node_indices = get_internal_node_idxs(tree)\n",
    "    # Compute the labelled spans for all internal nodes\n",
    "    labelled_spans = [compute_labeled_span_for_node(tree, idx) for idx in internal_node_indices]\n",
    "    return [string_for_lspan(label, left, right) for label, left, right in labelled_spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82213252",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_spans1 = get_labelled_spans_for_tree(example_trees[0])\n",
    "example_spans2 = get_labelled_spans_for_tree(example_trees[1])\n",
    "\n",
    "display(example_spans1)\n",
    "display(example_spans2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91215d6c",
   "metadata": {},
   "source": [
    "### Part 7.1: Parser evaluation (10 points)\n",
    "\n",
    "Do the following:\n",
    "- Use the Viterbi parser from the previous exercise to extract the parses for these sentences.\n",
    "- Finish the implementation for precision, recall and F1 score and calculate these metrics for the parser output against the two test sentences. Verify that you get the following scores:\n",
    "\n",
    "| Sentence        | Precision   | Recall | F1 |\n",
    "|:------------- |:-------------:|:------:|:--:|\n",
    "| `They will join the board .`      | 0.82 | 0.82 | 0.82 |\n",
    "| `The student will do the work on Monday .`    | 0.87  | 0.81 | 0.84 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f605ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 SOLUTION:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d256d8",
   "metadata": {},
   "source": [
    "### Part 7.1b (BONUS, 5 points):\n",
    "\n",
    "This question can lead to bonus points, which may compensate any error you made elsewhere.\n",
    "\n",
    "- What happens to the evaluation metrics if you switch the test tree and the parse tree around?\n",
    "- What happens if you compare the first test tree against the second parsed sentence and vice versa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0468c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1b SOLUTION:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b3267b",
   "metadata": {},
   "source": [
    "### Part 7.2: Large-Scale evaluation (10 points)\n",
    "\n",
    "Now we must find out a way to evaluate on a larger test set of sentences. This set is given in the file `test_trees.txt`, which is already loaded by the code below. Do the following:\n",
    "\n",
    "1. Parse each of the sentences.\n",
    "2. Calculate the micro-averaged precision, recall and F1 scores. That is, you must first aggregate all predictions and then calculate the overall metrics! This time, the trees in the test set correspond to the *correct parses*.\n",
    "\n",
    "*Note: running over all the test trees may take a while. Although it shouldn't take more than a few minutes at most, it is recommended to use `tqdm` when iterating over the test trees to keep track of the progress, as indicated in the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3230a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 SOLUTION:\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('test_trees.txt', 'r') as in_file:\n",
    "    test_trees = [Tree.fromstring(s.strip()) for s in in_file.readlines()]\n",
    "\n",
    "for tree in tqdm(test_trees, position=0):\n",
    "    print(\"Your solution awaits!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c988547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
